\chapter{Unexplored ideas}\label{sec:futurework}
\section{MIP model}
\subsection[Upper bound on $\Lmax$]{Upper bound on {\sansitalicfont L}\textsubscript{max}}
An upper bound on $\Lmax$ can be found by using a dispatch rule to find a
feasible, if not optimal, schedule. A good approach could be the ``best-fit''
heuristic proposed in Malapert's paper. 

\subsection[Bounding the number of batches $n_k$]{Bounding the number of batches
\sansitalicfont n\textsubscript{k}}\label{sec:bounding_nk}
Initially, the number of batches needed is assumed to be equal to the number of
jobs: $n_k = n_j$. Reducing $n_k$ by pre-computing the maximum number of batches
needed shrinks the $x_{jk}$ matrix, and prunes potential search branches in
branch-and-bound decomposition approaches.

\input{figures/bnk1_1050} %fig:bnk1 (50/10 example)

Unfortunately, we cannot make a general statement that optimal solutions never
have more batches than other feasible solutions -- a simple counterexample is
shown in figure \ref{fig:bnk1}.\footnote{To be more precise, we cannot state
that at least one optimal solution is in the subset of feasible solutions that
uses the fewest number of batches -- a dominance situation that could be
exploited, were it true.}

Starting out with a one-job-per-batch schedule sorted by EDD, we can explore
all feasible batch configurations recursively. To generate any other feasible
schedule (including the optimal solution), jobs $j$ are rescheduled (``moved back'')
from their original batch $k_\text{origin}$ into a prior batch
$k_\text{earlier}$. This eliminates $k_\text{origin}$ and requires, of course,
that $k_\text{earlier}$ has sufficient capacity. If the job's processing time
$p_j$ exceeds that of $k_\text{earlier}$, then the lateness of batches between
$k_\text{earlier}$ and $k_\text{origin}$ will increase; the merit of such a move
cannot be judged a priori, so it is an \textit{unsafe} batch elimination.
\textit{Safe} eliminations, on the other hand, will never worsen $\Lmax$, and
only they can be considered when bounding $n_k$ a priori.

Algorithm \ref{alg:bounding_nk} outlines a recursive method to find an upper
bound on $n_k$, recognizing safe batch eliminations only.

\begin{algorithm}[h]
\fontsize{9pt}{11.5pt}\selectfont
\begin{algorithmic}
\If{no open jobs left} \Comment{if this is a ``leaf node'' in the recursion}
  \State update UB$(n_k)$
\EndIf
\State find the combination of unsafe later jobs that fills up the capacity
most, leaving us with capacity $b_r$ 
\State find all combinations $x$ of safe later jobs that fit into $b_r$
\Repeat
  \State $x$ = next safe job combination
  \State \textit{ignoreJobs} $\gets x$ \Comment{make the moves, let the next recursion
  level deal with the rest of the jobs}
  \State spawn and run child node with $J \setminus$ \textit{ignoreJobs}
  \State \textit{ignoreJobs} $\gets$ \textit{ignoreJobs} $\setminus x$
\Until{all combinations have been explored}
\State return
\end{algorithmic}
\caption{Recursive algorithm to find an upper bound on $n_k$}
\label{alg:bounding_nk}
\end{algorithm}

This algorithm evidently requires exponential time. A relaxed variant is a
possible option: if only a single unsafe move \textit{into} a batch is possible,
no safe eliminations into that batch are considered at all and we skip to the
next batch. This would greatly speed up the recursion but also significantly
weaken the usefulness of the resulting upper bound.

In a branch-and-bound decomposition approach in which batches are modelled
individually, an upper bound on $n_k$ could be used to limit the depth of the
search tree, or, in combination with a method to determine a lower bound on the
remaining jobs' $n_k$ at every node, to actively prune the search tree during
the search. The latter method, however, would also run in exponential time as
it, again, would require knapsack-type reasoning unless we use a much less powerful
relaxation.

\section{CP Model}
\subsection[Constraint on the number of batches with length $P_k >
p$]{Constraint on number of batches with length {\sansitalicfont
P\textsubscript{k}} > {\sansitalicfont p}}

Since \marginpar{\em\small I haven't found an elegant way to turn this into a
gcc constraint, but I also fail to see how it would be useful -- this constraint
matches \textrm{all} feasible solutions, not just a superset of the optimal
one.} batches take on the processing time of their longest job, there is at
least one batch with $P = \underset{j}{\max} p_j $. We can proceed to fill batches with jobs, ordered by non-increasing processing
time, based on algorithm \ref{alg:findBatchlengthCards}. 

\begin{algorithm}[h!]
\fontsize{9pt}{11.5pt}\selectfont \begin{algorithmic} \State $J^{\star} \gets J$
\Comment{initialize all jobs as unassigned jobs} \State $n_k \gets 1$; $S_k
\gets \{0\}$; $P_{k,\text{min}} \gets \{0\}$ \Comment{Create one empty batch of
size and length zero} \State sort $J^{\star}$ by processing time, non-increasing
\Repeat \State $j \gets J^{\star}$.pop() \Comment{select job for assignment,
longest job first} \Loop $\;$ through all $n_k$ existing batches $k$, first
batch first \State $k_p \gets \emptyset$ \Comment{no feasible batch} \State
$c_\text{min} = b$ \Comment{currently known minimum remaining capacity} \If{$s_j
< b-S_k$ and $b-S_k < c_\text{min}$} \State $k_p \gets k_p$; $c_\text{min} \gets
b-S_k$ \EndIf \EndLoop \If{$|k_p| = 1$} \State $S_{k_p} \gets S_{k_p} + s_j$
\Comment{assign job $j$ to batch $k_p$} \Else \If{$n_k < LB(n_k)$} \State $n_k
\gets n_k + 1$\Comment{open new batch} \State $S_{n_k} \gets s_j$;
$P_{n_k,\text{min}} \gets p_j$ \Comment{assign $s_j$ and $p_j$ to the new batch}
\Else \State leave the loop now and end.  \EndIf \EndIf \Until{$J^{\star}$ is
empty} \end{algorithmic} \caption{Generating lower bounds on batch lengths}
\label{alg:findBatchlengthCards} \end{algorithm}
At the end of this algorithm, we can state: \begin{alignat}{2} &
\mathtt{globalCardinality}( |P_{k-1,\text{min}} > P_k \geq P_{k,\text{min}} |
\geq 1) \quad && \forall k \in \{k_0,\dots,k_{LB(n_k)}\} \end{alignat}

The algorithm sorts jobs by non-increasing $p$, and then fills batches job by
job. If a job fits into a previous batch, it is assigned there. If a job fits
into multiple previous batches, it is assigned to the batch with the smallest
remaining capacity. This is called \textit{best-fit decreasing} rule,
and works as follows: let $J^\star$ be the set of jobs sorted by $p$, then at
least one batch will be as long as the longest job $j^\star_1$. If the next $n$
jobs fit into this batch, then there is at least one batch not shorter than
$j^\star_{n+1}$, and similarly for subsequent batches. 

Unfortunately, the optimal solution may perform better than the packing heuristic in
terms of ``vertical'' ($s_j$) bin packing, and may thus require fewer batches.
We therefore need to find a lower bound $LB(n_k)$ on the number of batches, and
we can only guarantee the first $LB(n_k)$ of the above constraints to hold in
the optimal solution. Finding a true lower bound is a two-dimensional bin
packing problem, which runs in exponential time, so we
have to come up with an even lower bound -- right now I can only think of $j_0$,
the number of jobs ordered by decreasing $s_j$ that can never fit into a batch
together.

\subsection[Constraint on the number of batches with due date $D_k >
d$]{Constraint on number of batches with length {\sansitalicfont
D\textsubscript{k}} > {\sansitalicfont d}}

In a similar fashion, we can determine that the second batch must be due no
later than the earliest-due job $j_{m+1}$ that can \textit{not} fit into the first
batch -- if we sort jobs by due date and fit the earliest $m$ jobs into the first
batch -- and so on for subsequent batches. Once again, since best-fit decreasing
may not perform optimally in terms of ``vertical'' packing, this may not be
valid for batches beyond the known $LB(n_k)$.

\subsection[All-different constraints on $P_k$ and $D_k$]{All-different
constraints on {\sansitalicfont P\textsubscript{k}} and {\sansitalicfont
D\textsubscript{k}}}

Furthermore, if all jobs have different processing times, \marginpar{\em\small I haven't found a way to implement this efficiently in CP
Optimizer; I also don't quite see how it would constrain anything.}all batches will have different
processing times as well: \texttt{alldifferent}$(P_k)$. If $m$ out of $n_j$ jobs
have different processing times, we can still enforce
\texttt{k\_alldifferent}$(P_k, m)$. 

Similarly, we knows that the constraint \texttt{k\_alldifferent}$(D_k, m)$ must
be true if $m$ out of $n_j$ jobs have different due dates.

\section{Decompositions}
\subsection{Potential heuristics}
\paragraph{Improve the initial $L_{\text{max,incumbent}}$} A better initial
upper bound on $\Lmax$ can help prune some branches of the search tree from the
outset. There are several dispatch rules (or maybe other heuristics?) that could
be explored to do this better.
\paragraph{Improve $L_{\text{max,incumbent}}$ during search} It may be useful to
use a heuristic like above to ``complete the schedule'' once a promising partial
schedule has been generated. I have yet to identify situations where this is
always helpful.

\subsection{Move-back decomposition}

Another possible way to set up a branch-and-bound search for solutions works as
follows: first, consider all jobs to be ``single'', i.e. assigned to individual
batches such that $B_j = k_j \; \forall j \in J$. Compute the $\Lmax$ for this
schedule. Then, at every level of the search tree, move some single job $j$ into any
earlier batch $k \leq k_j$, but only if that move does not violate $k$'s
capacity. 

If we start with a schedule of single jobs and only allow moving single jobs
into earlier batches (and any schedule can be produced by a sequence of such
moves!), we maintain the EDD ordering of batches. More importantly, such moves
will never shift the position of $\Lmax$ to the right:

In any partial schedule following EDD, let $k$ be the batch
with maximum lateness $L_k = \Lmax$. It has processing time $p_k$. Then the lateness of the
batch before $k$ must be $L_{k-1} \geq L_k - p_k$ as a consequence of the EDD
sequencing. Any batch following $k$ can have a lateness no greater than $L_k -
1$.

\begin{alist}
\item{Moving any single job from a batch following $k$ into a batch before $k$ will
worsen $\Lmax$, but not change its position. Such a move is never necessary to
arrive at an optimal solution.}
\item{Moving back any single job from a
batch before $k$ \textit{safely}\footnote{for the meaning of ``safe'' and
``unsafe'' moves, compare section \ref{sec:bounding_nk}.} will improve $\Lmax$,
but not change its position.}
\item{Moving back a single pre-$k$ job $j$ from a batch
$\beta$ into an earlier batch $\alpha$ \textit{unsafely} will reduce $L_k$ by
$p_\alpha$; $\Lmax$ may still be in $k$, or it may be found in any batch between
(and including) $\alpha$ and $\beta$, since their lateness
$L_{[\alpha,\dots,\beta]}$ is now increased by $p_j - p_\alpha$.}
\item{If the
max-lateness job $j$ is single itself, it can be moved from $k$ into an earlier
batch $\alpha$. If this is done \textit{safely}, the batch immediately preceding
$k$ will still have a lateness $L_{k-1} \geq L_k - p_j$. All batches after $k$
will have their lateness reduced by $p_j$, but since their maximum lateness did
not exceed $L_k - 1$ before the move, it will now be at least 1 less than that
of batch $k-1$.}
\item{If the max-lateness job
$j$ was moved back \textit{unsafely} into a batch $\alpha$, batches between and
including $\alpha$ and $k-1$ now have their lateness increased by $p_j - p_\alpha$,
while batches after $k$ have their lateness decreased by $p_\alpha$. Again,
batch $L_{k-1}$ would exceed the maximum lateness of batches after $k$.}
\end{alist}

This shows that after a sequence of operations in which single jobs are moved into
earlier batches, $\Lmax$ will never shift to the right. In fact, this means that
all jobs after the $\Lmax$-job in a single-job EDD schedule can be ignored in
the scheduling problem entirely, although this turns out to be quite
inconsequential as that job is often at or near the end of the single-job EDD
schedule, resulting from the fact that $L_{k-1} \geq L_k - p_k$. By the same
token, high-quality solutions often have their $\Lmax$ in an early batch. This
may give rise to some sort of decomposition method in which jobs are
strategically moved back, and where we are trying to move $\Lmax$ to the left as
much as possible. 
