\documentclass[13pt, letterpaper, twoside]{book}
\usepackage{graphicx}

\usepackage[driver=xetex,paperwidth=8.5in,paperheight=11in,left=1.1in, right=1.74in,top=1.4in, bottom=1.74in]{geometry}
\usepackage[no-math]{fontspec}

\usepackage{sectsty, tikz, color}
\usepackage{multicol}
\usepackage{amsmath, amssymb, amsfonts,  titlesec}
\usepackage[urw-garamond]{mathdesign}
\usepackage{fancyhdr, booktabs}
%\usepackage[font=small,format=plain,labelfont=it,textfont=it]{caption}
\usepackage{caption,subcaption}
\usepackage{listings}
\usepackage{algpseudocode, algorithm}
% \usepackage[T1]{fontenc} 
\usepackage{enumitem}

 \DeclareTextCommandDefault{\nobreakspace}{\leavevmode\nobreak\ } 
\include{typography}

\usepackage[pdfpagelayout=TwoPageRight, hidelinks]{hyperref}

\begin{document}
\frontmatter

\include{coverpage}
%\include{firstpage}

\tableofcontents

\mainmatter
\pagebreak
\vskip 4em
\chapter{Introduction}

\chapter{Fundamentals}
Some introductory paragraphs on minimization/maximization problems.
\section{Linear programming models}
An introduction to and illustration of LP models.
\subsection{Problem motivation}
\subsection{Graphical explanation}
\section{Integer programming models}
An introduction to and illustration of MIP models
\subsection{Solution technique}
\subsection{Other methods}
\section{Constraint programming models}
\subsection{Concept}
\subsection{Solution}
\subsection{Global constraints}

\chapter{Modelling the problem}
\section{Characteristics of the problem}

...

\section{Bounds on variables known a priori}

\subsection{Lower bound on {\sansitalicfont L}\textsubscript{max}}
A lower bound on $\Lmax$ can be found using the lower bound on the completion
date of each bucket $q$, where a bucket is defined as the set of batches with
due date $\delta_q$:
\begin{alignat}{2}
& \Lmax \geq C_{\text{max},q} - \delta_q \quad && \forall q
\end{alignat}
This works because the buckets up to bucket $q$ are guaranteed to contain all
jobs with due dates $d \leq \delta_q$, since the batches within the buckets are
ordered by earliest due date (EDD) in the optimal solution. The buckets up to
bucket $q$ will likely also contain some later ($d > \delta_q$) jobs in the
optimal solution but this does not affect the validity of the lower bound.

%To find a lower bound on $C_{\text{max},q}$, we can exploit the fact that every
%batch needs to span its jobs. If we order the jobs up to bucket $q$ by non-increasing processing
%time $p$, then we can use algorithm \ref{alg:findcmax} to find
%$C_{\text{max},q}$:
%\begin{algorithm}
%\begin{algorithmic}
%\State $J^{\star} \gets J$ \Comment{initialize all jobs as unassigned jobs}
%\State $n_k \gets 1$; $S_k \gets \{0\}$; $P_k \gets \{0\}$ \Comment{Create one
%empty batch of size and length zero}
%\State sort $J^{\star}$ by processing time, non-increasing
%\Repeat
%  \State $j \gets J^{\star}$.pop() \Comment{select job for assignment, longest job
%first}
%  \Loop $\;$ through all $n_k$ existing batches $k$, first batch first
%    \State $k_p \gets \emptyset$ \Comment{no feasible batch}
%    \State $c_\text{min} = b$ \Comment{currently known minimum remaining
%    capacity}
%    \If{$s_j < b-S_k$ and $b-S_k < c_\text{min}$}
%      \State $k_p \gets k_p$; $c_\text{min} \gets b-S_k$
%    \EndIf
%  \EndLoop
%  \If{$|k_p| = 1$}
%      \State $S_{k_p} \gets S_{k_p} + s_j$ \Comment{assign job $j$ to batch $k_p$}
%  \Else
%    \State $n_k \gets n_k + 1$\Comment{open new batch}
%    \State $S_{n_k} \gets s_j$; $P_{n_k} \gets p_j$ \Comment{assign $s_j$ and $p_j$ to the new batch}
%  \EndIf
%\Until{$J^{\star}$ is empty}
%\end{algorithmic}
%\caption{Finding $C_{\text{max},q}$}
%\label{alg:findcmax}
%\end{algorithm}
%
%The algorithm is based on the following reasoning: no matter the assignment, one
%batch will have the length $P_k$ of the longest job $\max(p_j)$.
%Since we ordered and assigned the jobs by $p_j$, this will be the first batch
%$k_1$.
%Continue filling $k_1$ until some job $j_c$ exceeds capacity. This job
%must go into another batch $k_2$. Swapping $j_l$ with any of the already assigned jobs
%in $k_1$ (capacity allowing) increases $C_\text{max}$. Even if ``leaving
%capacity'' in $k_1$ for later, better-fitting open jobs may seem desirable --
%these other open jobs will necessarily be shorter than $j_c$, so such a move
%will again serve to increase $C_\text{max}$. Open jobs that fit into
%previous batches with some remaining capacity should be assigned to the batch
%with the \textit{minimum} feasible remaining capacity $c_\text{min}$. This
%avoids unnecessarily precluding shorter (still open) but slightly larger jobs
%from being assigned to previous batches wherever possible. 
%
%A combination of shorter jobs may be a better fit to a previous batch than a
%single, longer job in terms of using remaining capacity, but what matters is
%only the processing time.
%

Now we need to find $C_{\text{max},q}$, or at least a lower bound on it, in
polynomial time. The simplest approach simply considers the ``total elastic area'',
i.e. the sum of all $s_j p_j$ products:
\begin{alignat}{2}
& C_{\text{max},q} \geq \big\lceil\frac{1}{b} \sum_{j : d_j \leq \delta_q} s_j
p_j\big\rceil \quad
&& \forall q
\end{alignat}
A better lower bound on $C_{\text{max},q}$ would be given by a
preemptive-cumulative schedule. Unfortunately, minimizing $C_{\text{max}}$ for
such problems is equivalent to solving a standard bin-packing problem, which
requires exponential time. 

\subsection{Upper bound on {\sansitalicfont L}\textsubscript{max}}
An upper bound on $\Lmax$ can be found by using a dispatch rule to find a
feasible, if not optimal, schedule. A good approach could be the ``best-fit''
heuristic proposed in the original paper. {\color{darkred} This has not been
implemented yet.}

\subsection{Bounding the number of batches \sansitalicfont n\textsuperscript{k}}
Initially, the number of batches needed is assumed to be equal to the number of jobs: $n_k = n_j$. Reducing $n_k$ by pre-computing the maximum number of batches needed shrinks the $x_{jk}$ matrix.

Unfortunately, we cannot make a general statement that optimal solutions never have more batches than other feasible solutions -- a simple counterexample is shown in figure \ref{fig:bnk1}.\footnote{To be more precise, we cannot state that at least one optimal solution is in the subset of feasible solutions that uses the fewest number of batches -- a dominance situation that could be exploited, were it true.}

. . .

. . .
\vfil
\input{figures/bnk1_1050}

{\color{darkred} I have a long list of ideas here, none of which I've been able
to prove right or wrong, so I've commented them out and made this page end here.}

%It is perhaps possible, however, to generate a feasible solution that is likely to use $n_k < n_j$ and to guarantee that such a solution will never use fewer batches than the optimal solution. To do this, let the initial solution $\pi_0$ be a schedule in which every job is assigned to one batch (i.e. $n_k = n_j$) and the jobs are ordered by non-decreasing due date (i.e. $d_n \leq d_{n+1}$), as in figure \ref{fig:bnk2}.
%
%\input{figure showing all the jobs in EDD sequence}
%
%Capacity permitting, jobs are now moved into earlier batches to improve the schedule, eliminating the batch they were placed in initially. Every such move reduces $n_k$ by one.
%
%Two types of moves are possible: \textit{safe} moves and \textit{risky} moves. A move is safe when a job is moved into an earlier batch of longer processing time. A move is risky when a job is moved into an earlier batch of shorter processing time, thus increasing the lateness of that batch.
%
%Moving a job has three effects: 
%
%\begin{alist}
%\item{Effect on the batches after}
%\item{Effect on the batches between, including the new job}
%\item{Effect on the job itself}
%\end{alist}
%
%\begin{table}
%\centering
%\begin{tabular}{r c c}
%\toprule
%              & safe                 & risky \\
%\midrule
%batches after & lateness improves by $p_j$ & lateness improves by $p_a$ \\
%batches between & --- & lateness worsens by $p_j - p_a$ \\
%job itself & lateness improves & lateness improves\\ 
%\bottomrule
%\end{tabular}
%\caption{Lateness effects of safe and risky moves}
%\end{table}
%
%We can easily generate a solution that only contains safe moves.
%We now need to prove that our relaxed solution $\pi_{\text{edd}}$ will never use fewer batches than the optimal solution $\pi_{\text{opt}}$.
%
%For this to be true, we need to show that if there is ever a situation in which we could \textit{either} make $n$ unsafe moves \textit{or} $m>n$ safe ones, using the unsafe ones will never be beneficial if the remaining $m-n$ safe candidates cannot be moved somewhere else.
%
%We're trying to find a relaxation of the problem that is guaranteed to generate as least as many batches as the optimal solution.
%
%
%Whenever there is an alternative between $n$ unsafe moves and $>n$ safe ones, perform $n$ safe ones. Whenever only an unsafe is available, eliminate none and proceed to the next batch.
%
%Unfortunately, we cannot generally state that optimal solutions never have more batches than other feasible solutions: when a job $j_b$ is moved into a prior batch holding $j_a$ and $d_b > d_a$ as well as $p_a < p_b$, it can sometimes happen that $\Lmax$ is increased.
%
%\subsubsection{Some stuff to try}
%
%We can try only considering jobs \textit{before} $\Lmax$, because anything after that job wouldn't be moved in an optimal solution. 
%
%

\section{MIP model used}
\subsection{Original approach}
This is Malapert's original approach:
\begin{alignat}{2}
  &\sum_{k \in K} x_{jk} = 1 \quad && \forall j \in J \\
  &\sum_{j \in J} s_j x_{jk} \leq b \quad && \forall k \in K\\
  &p_j x_{jk} \leq P_k \quad && \forall j \in J, \forall k \in K\\
  &C_{k-1} + P_{k} = C_k \quad && \forall k \in K\\
  &(d_{max} - d_j)(1 - x_{jk}) + d_j \geq D_k \quad && \forall j \in J, \forall k \in K\\
  &D_{k-1} \leq D_k \quad && \forall k \in K \label{malapp-edd} \\
  &C_k - D_k \leq \Lmax \quad && \forall k \in K\\[2ex]
  &C_k \geq 0, P_k \geq 0 \text{ and } D_k \geq 0 \quad && \forall k \in K  
\end{alignat}

The constraint \ref{malapp-edd} is the EDD constraint.

\subsection{Grouping empty batches}
The given formulation lacks a rule that ensures that no empty batch is followed by a non-empty batch. Empty batches have no processing time and a due date only bounded by $d_\text{max}$, so they can be sequenced between non-empty batches without negatively affecting $\Lmax$. Since, however, desirable schedules have no empty batches scattered throughout, we can easily reduce the search space by disallowing such arrangements. The idea is illustrated in Figure \ref{fig:dominancerule}.

\input{figures/dominancerule}

A mathematical formulation is
\begin{alignat}{2}
& \sum_{j \in J} x_{j,k-1} = 0 \rightarrow \sum_{j \in J} x_{jk} = 0 \quad && \forall k \in K. \label{eq:emptybatch0}
\end{alignat}

To implement this, we can write constraints in terms of an additional  set of binary variables, $e_k$, indicating whether a batch $k$ is empty or not:

\begin{alignat}{2}
& e_k + \sum_{j \in J} x_{jk} \geq 1 \quad && \forall k \in K, \label{eq:emptybatch1} \\
& n_j (e_k-1) + \sum_{j \in J} x_{jk} \leq 0 \quad && \forall k \in K. \label{eq:emptybatch2}
\end{alignat}

Constraint \ref{eq:emptybatch1} enforces $e_k = 1$ when the batch $k$ is empty. Constraint \ref{eq:emptybatch2} enforces $e_k = 0$ otherwise, since the sum term will never exceed $n_j$. The rule \ref{eq:emptybatch0} can now be expressed as $e_{k-1} = 1 \rightarrow e_k = 1$, and implemented as follows:

\begin{alignat}{2}
& e_k - e_{k-1} \geq 0 \quad && \forall k \in K.
\end{alignat}

We can also prune any attempts to leave the first batch empty by adding a constraint $e_0 = 0$.

{\color{darkred} This, while it works just fine, actually takes longer than without it.}

\subsection{No postponing of jobs to later batches}
Since the jobs are already sorted by non-decreasing due dates, it makes sense to explicitly instruct the solver never to attempt to push jobs into batches with a greater index than their own: even if every job had its own batch, it would be unreasonable to ever postpone a job to a later batch.
\begin{alignat}{2}
  & x_{jk} = 0 \quad && \forall j,k : j > k
\end{alignat}



\section{CP model}

[Some introduction ...]

\subsection{Bin-packing and cumulative constraints}
This makes sure the jobs are distributed into the batches such that no batch
exceeds the capacity $b$:
\begin{align}
\mathtt{pack}(J, K, b)
\end{align}

The cumulative constraint functions similarly, but instead of packing
discrete bins, it enforces non-overlapping constraint on the temporal
(\texttt{IntervalVariable}) $J$ variables. {\color{darkred} For this to work
directly with the \texttt{assignments} array used by \texttt{pack} (i.e. without
a $x_{jk}$ matrix), it seems I need to define my own search goal class -- I
still need to do this).}
\begin{align}
\mathtt{cumul}(J, b)
\end{align}

\subsection{Temporal constraints}
These constraints are implemented using \texttt{IntervalVar} objects, which
offer properties such as \texttt{lengthOf} or \texttt{endOf}.

\begin{alignat}{2}
& P_k \geq \underset{j : B_j = k}{\max} \; p_j \quad &&
\forall k \in K \\
& D_k \leq \underset{j : B_j = k}{\min}\; d_j \quad && \forall k \in K \\
& C_k + P_{k+1} = C_{k+1} \quad && \forall k \in K \\
& \Lmax \geq \underset{k}{\max} \;(C_k - D_k) && 
\end{alignat}

The first constraint ensures that each batch is as long as its longest job. 
The second constraint ensures that the earliest-due job sets the due date.
The third constraint defines batch completion dates, and the fourth constraint defines $\Lmax$.

The first two temporal constraints were {\color{darkred}(rather, will be)}
implemented in custom constraint propagator classes using the job assignments
propagated by the bin-packing constraint. They exploit the temporal features of
\texttt{IntervalVar}s.

\subsection{Constraint on number of batches with length
{\sansitalicfont P\textsubscript{k}} > {\sansitalicfont p}}

Since batches take on the processing time of their longest job, there is at
least one batch with $P = \underset{j}{\max} p_j $:
\begin{align}
\mathtt{globalCardinality}( |P_k = \underset{j}{\max} \; p_j| = 1 ) 
\end{align}
We can proceed to fill batches with jobs, ordered by non-increasing processing
time, based on algorithm \ref{alg:findBatchlengthCards}.
\begin{algorithm}
\fontsize{9pt}{11.5pt}\selectfont
\begin{algorithmic}
\State $J^{\star} \gets J$ \Comment{initialize all jobs as unassigned jobs}
\State $n_k \gets 1$; $S_k \gets \{0\}$; $P_{k,\text{min}} \gets \{0\}$ \Comment{Create one
empty batch of size and length zero}
\State sort $J^{\star}$ by processing time, non-increasing
\Repeat
  \State $j \gets J^{\star}$.pop() \Comment{select job for assignment, longest job
first}
  \Loop $\;$ through all $n_k$ existing batches $k$, first batch first
    \State $k_p \gets \emptyset$ \Comment{no feasible batch}
    \State $c_\text{min} = b$ \Comment{currently known minimum remaining
    capacity}
    \If{$s_j < b-S_k$ and $b-S_k < c_\text{min}$}
      \State $k_p \gets k_p$; $c_\text{min} \gets b-S_k$
    \EndIf
  \EndLoop
  \If{$|k_p| = 1$}
      \State $S_{k_p} \gets S_{k_p} + s_j$ \Comment{assign job $j$ to batch $k_p$}
  \Else
    \If{$n_k < LB(n_k)$}
      \State $n_k \gets n_k + 1$\Comment{open new batch}
      \State $S_{n_k} \gets s_j$; $P_{n_k,\text{min}} \gets p_j$ \Comment{assign $s_j$ and $p_j$ to the new batch}
    \Else
      \State leave the loop now and end.
    \EndIf
  \EndIf
\Until{$J^{\star}$ is empty}
\end{algorithmic}
\caption{Generating lower bounds on batch lengths}
\label{alg:findBatchlengthCards}
\end{algorithm}

At the end of this algorithm, we can state:
\begin{alignat}{2}
& \mathtt{globalCardinality}( |P_{k-1,\text{min}} > P_k \geq P_{k,\text{min}} | =
1) \quad && \forall k \in \{k_0,\dots,k_{LB(n_k)}\}
\end{alignat}

The algorithm sorts jobs by non-increasing $p$, and then fills batches job by
job. If a job fits into a previous batch, it is assigned there. If a job fits
into multiple previous batches, it is assigned to the batch with the smallest
remaining capacity. {\color{darkred} As I found out today, this is called
\textit{best-fit decreasing rule}. I can expand on why this works, if
necessary.}

Unfortunately, the optimal solution may perform better in terms of ``vertical''
($s_j$) bin packing, and may thus
require fewer batches. We therefore need to find a lower bound $LB(n_k)$ on the
number of batches, and we can only guarantee the first $LB(n_k)$ of the above
constraints to hold in the optimal solution. Finding a true lower bound is a
two-dimensional bin packing problem, which runs in exponential time
{\color{darkred} $\dots$ so we have to come up with an even lower bound -- right
now I can only think of $j_0$, the number of jobs ordered by decreasing $s_j$
that can never fit into a batch together.}

{\color{darkred} This whole method can surely be
improved somehow by realizing a priori that some jobs must go before others?}

Furthermore, if all jobs have different processing times, all batches will have
different processing times as well: \texttt{alldifferent}$(P_k)$. If $m$ out of $n_j$ jobs have different
processing times, we can still enforce \texttt{k\_alldifferent}$(P_k, m)$.
Propagation rules for this constraint were given in \cite{Lardeux}.
{\color{darkred} I can't find anything on this w.r.t. CP Optimizer, so I may
have to implement this myself $\dots$ time permitting.} 

\subsection{Temporal constraints on a job's start date}
Given any partial assignment of jobs and an open job $j$, we can reason that
\begin{alist}
\item{if the first batch with a due date later than the job is $k$, then the job
cannot be part of a batch after $k$ -- this would result in a non-EDD sequence
of batches.}
\item{if the first batches up to $k-1$ offer not enough capacity for $j$ due to
the given partial assignment, then the job cannot be part of a batch before
$k$.}
\end{alist}
Since batches are \textit{not} dynamically created like in Malapert's solution
but fixed from the start, any partial assignment that fails due to these
constraints cannot be part of an optimal solution.

This constraint is redundant with both the $(C_{k+1}\geq C_k)$ and
\texttt{packing} constraints, but may help accelerate the propagation in some
cases.

\subsection{Restricting the search to {\sansitalicfont before
L}\textsubscript{max}}
Once a feasible (but possibly non-optimal) solution is found that has $\Lmax$ in
batch $k_{\Lmax}$, then this solution dominates all solutions with other job
configurations 
\textit{after} $k_{\Lmax}$ -- improving $L$ in those batches has no impact on $\Lmax$,
and worsening it never leads to a better solution.

Therefore, once all jobs are assigned to a feasible solution, the search should be limited
to the following jobs:
\begin{alist}
\item{those jobs in the first batch that are due with the second batch $k_2$ or
later,}
\item{all jobs in batches $\{k_2, \dots, k_{\Lmax-1}\}$,}
\item{all jobs in $k_{\Lmax}$ for which $p_j > j_{\Lmax}$,}
\item{$j_{\Lmax}$ itself, but it can only be scheduled earlier.}
\end{alist}

Once another, better feasible solution is found, this constraint on
``searchable'' variables must be updated. If no better solution is found using
those variables, the solution is optimal. {\color{darkred} I think this could
maybe speed up the search, but I have no idea how to implement this dominance
rule as a constraint. The documentation offers \texttt{SearchPhase} functions
that can guide the search, but I haven't experimented with that yet.}

\subsection{Grouping empty batches}
Just like in the MIP model, we can force empty batches to the back and thus
establish dominance of certain solutions. The implementation is much easier than
in the MIP model:
\begin{alignat}{2}
& \mathtt{IfThen}( C_{k+2} > C_{k}, C_{k+1} > C_{k} ) \quad && \forall k \in
\{k_1, \dots, k_{n_k-2}\}
\end{alignat}

\subsection{No postponing of jobs to later batches}
Just like in the MIP model, jobs should never go into a batch with an index
greater than their own:
\begin{alignat}{2}
  & x_{jk} = 0 \quad && \forall j,k : j > k
\end{alignat}
This is implemented as $\mathtt{assignments}_j \leq k$. 

{\color{darkred} This constraint is analogous to the concept of finding an upper
bound on $n_k$ -- essentially, it would be very helpful to find an upper bound
on the latest batch for \textit{every} job.}

%\section{Other known properties, unused so far}
%Anything that comes \textit{after} $\Lmax$ doesn't matter. If $L$ is decreasing after $\Lmax$, the jobs must not be stacked ever. In other words: don't ever consider an assignment where 

\section{Open questions}
\begin{alist}
\item{Is there a dominance rule that reduces the size of the problem because two
jobs can be ``fused together'' (WHATS THE WORD?) to be considered as one?}
\end{alist}

\section{Test data for evaluation}
Malapert uses benchmark data by Daste. For design purposes, I created my own set of randomized job lists, with $s_j, p_j \in [1, 20]$ and $d_j \in [1, 10n]$ where $n$ is the number of jobs. Here is the Python code used:

\lstset{language=Python}
\begin{lstlisting}
import csv
import random

random.seed(None)

for i in range(10,100,20):
    with open( "data_" + str(i), "wb") as csvfile:
        csvwriter = csv.writer(csvfile, delimiter=",")
        csvwriter.writerow(['s_j', 'p_j', 'd_j'])
        for job in range(i):
            csvwriter.writerow([random.randrange(1,20),
               random.randrange(1,20),
               random.randrange(1,5*i)])
    csvfile.close()
\end{lstlisting}

CSV files can be read in with \texttt{IloCsvReader} and \texttt{IloCsvLine}.


\chapter{My solution}
\section{MIP formulation improvements}
\section{CP formulation improvements}

\chapter{Discussion}


\pagebreak

\vskip 4em
\section[Intro]{Introduction}
\vspace{6.6em}

Books to read: Model Building in Mathematical Programming by HP Williams

Mathematical programming models all involve optimization. They want to minimize or maximize some objective function.

There are linear programming models, non-linear programming models and integer programming models.

Constraint programming and Integer Programming are twins and can usually be translated into one another. In CP each variable has a finite domain of possible values. Constraints connect/restrict the possible combinations of values which the variables cantake. These constraints are of a richer variety then the linear constraints of IP and are usually expressed in the form of predicates, such as ``all\_different(x1, x2, x3)''. The same condition could be imposed via IP, but it's more troublesome to formulate. Once one of the variables has been set to one of the values in its domain (either temporarily or permanently), this predicate would imply that this value must be taken out of the domain of all other variables (``propagation''). In this way constraint propagation is used until a feasible set of values is found (or not).

CP is useful where a problem function has no objective function and we are just looking for a feasible solution. We can't prove optimality, although we could using IP.

Comparisons and connectionsbetween IP and CLP are discussed by Barth (1995), Bockmayr and Kasper (1998), Brailsford
et al.
(1996), Prolland Smith (1998), Darby-Dowman and Little (1998), Hooker (1998) and Wilson and Williams (1998). Theformulation of the all\_different predicate using IP is discussed by Williams and Yan (1999).

The first approach to solving a multi-objective problem is to solve the model a number of times with each objective function in turn. The result may give an idea of what to do next.


\subsection{Chapter 8: Integer Programming}

The real power of IP as a method of modelling is that of binary constraints, where variables can take on values of 0 or 1. Maybe it should be called ``discrete programming.'' Precise definitions of those problems that can be formulated by IP models are given by Meyer (1975) and Jeroslow (1987). 

Problems with discrete inputs and outputs are the most obvious candidates for IP modelling (``lumpy inputs and outputs''). Sometimes solving an LP and rounding to the nearest integer works well, but sometimes it doesn't, as demonstrated in Williams, first example in 8.2. The smaller the variables (say, <5), the greater significance those rounding problems will have. 

When input variables have small domains, say, machine capacities, then again this may rule out LP relaxations. A good example of an IP problem is the knapsack assignment problem. A single constraint is that the capacity of the knapsack cannot be exceeded. Any LP formulation would always fill the knapsack 100\%, ignoring the discrete size of the objects. Two other well-known types of problems include \textit{set partitioning problems} and \textit{aircrew scheduling problems}. 

Integer programming models cannot be solved directly, but need to be brute-forced in a tree search manner. The search space, however, can be greatly reduced by a number of bounding techniques often depending on the nature of the problem. That is why the general method of IP solving is called \textit{branch-and-bound}.

So-called \textit{cutting planes methods} usually start by solving an IP problem as LP. If the resulting solution is integral, we're happy. Otherwise extra constraints (cutting planes) are added to the problem, further constraining it until an integer solution is found (or, if none can be found, we're out of luck). Cutting planes make for nice illustrations they are not very efficient with large problems. Cutting planes were invented by Gomory (1958).

Enumerative methods are generally applied to pure binary problems, where the search tree is pruned. The best known of these methods is Balas's additive algorithm described by Balas (1965). Other methods are givenby Geoffrion (1969). A good overall exposition is given in Chapter 4 of Garfinkel and Nemhauser (1972).

There are so-called \textit{pseudo-boolean methods} to solve pure binary problems that take boolean constraints as inputs. That may be comfortable for the user sometimes, but is rarely used in practice.

Generally, branch-and-bound methods first solve the LP relaxation to check whether we're lucky enough to find an integer solution. If not, a tree search is performed. 

\subsection{Chapter 9: Building IP models I.}

Binary variables are often called 01-variables. Decision variables could also have a domain like $\{0,1,2\}$ or $\{4,12,102\}$. Decision variables, especially the 01 kind, can be linked to the state of continuous variables like this: $x-M\delta \leq 0 \leftrightarrow x>0 \rightarrow \delta = 1$, where we know that $x < M$ is always true.

To use a 01 variable to indicate whether the following is satisfied:
\begin{align}
2x_1 + 3x_2 &\leq 1\\
x_1 &\leq 1\\
x_2 &\leq 1,
\end{align}
so, mathematically speaking,
\begin{align}
\delta = 1 &\rightarrow 2x_1 + 3x_2 \leq 1\\
2x_1 + 3x_2 \leq 1 &\rightarrow \delta = 1,
\end{align}
Then we can argue that at most, $2x_1 + 3x_2 = 5$, so
\[
2x_1 + 3x_2 + 4\delta \leq 5
\]
will ensure that $\delta = 1$ forces the equation to be true: use $M = 2 + 3 - 1$ to find $4$. In order to enforce $\delta = 1$ if the equation is true, use $m = 0 + 0 -1$ and write
\[
2x_1 + 3x_2 + \delta \geq 1
\]

All kinds of logical conditions can be modelled using 01 variables, although it's not always obvious how to capture them in that format. 

Logical conditions are sometimes expressed within a Constraint Logic Programming language as discussed inSection 2.4. The tightest way of expressing certain examples using linear programming constraints is described byHooker and Yan (1999) and Williams and Yan (1999). There is a close relationship between logic and 01 integerprogramming, which is explained in Williams (1995), Williams and Brailsford (1999) and Chandru and Hooker(1999).

\paragraph*{Special Ordered Sets}
Two very common types of restriction arise in mathematical programming, so two concepts (SOS1 and SOS2) have been developed. An SOS1 is a set of variables within which exactly one variable must be non-zero and the rest zero. An SOS2 is a set where at most two can be non-zero, and the two variables must be adjacent in the input ordering. Using a branch-and-bound algorithm specialized for SOS1 or SOS2 sets can speed things up greatly.

\paragraph*{Disjunctive constraints}
It is possible to define a set of constraints and postulate that at least one of them be satisfied.

\subsection{Special kinds of IP models}
Most practical IP models do not all into any of these categories but arise as MIP models often extending an existing LP model. Here are some examples.

\paragraph*{Set covering problems} We have a set $S = \{1,2,3,\dots,m\}$. We have a bunch of subsets $\mathcal{S}$ of subsets, each associated with a cost. Now cover all members of $S$ using the least-cost members of $\mathcal{S}$.

\paragraph*{Knapsack problem} These are the really simple ones with just one constraint, namely the constraint of not being able to take more items with you than the knapsack can carry while maximizing the value of the taken objects.

\paragraph*{Quadratic assignment problem} Two sets of objects $S$ and $T$ of the same size require the objects to be matched pairwise. There are costs associated with pairs of pairs, that is the cost $c_{ij,kl}$ is the cost of assigning $i$ to $j$ while also assigning $k$ to $l$. This cost will be incurred if both $\delta_{ij}$ and $\delta_{kl}$ are \texttt{true}, i.e. $\delta_{ij}\delta_{kl} = 1$. The objective function is a quadratic expression in 01 variables:
\begin{align}
\mathrm{Minimize}\;\sum^n_{\substack{i,j,k,l=1\\k>l}} c_{ij,kl}\delta_{ij}\delta_{kl}
\end{align}
The quadratic version is practically unsolveable, so to be able to enumerate the possible assignments the above objective function has to be split up into separate constraints, which obviously means there will be an explosion in problem size as the number of variables grow.

\subsection{How to formulate a good model}
According to Williams, it is easy to build IP models that, while correct, are really inefficient. Fortunately, with some knowledge of what happens behind the scenes (and some practice), sucky models can often be improved. One good method to know (although it doesn't help by itself) is to turn a general integer variable into a bunch of 01 variables. Say $\gamma$ is a general, non-negative integer variable with a known upper bound of $u$, then we can replace it with $\delta_0 + 2\delta_1 + 4\delta_2 + 8\delta_3 + \dots + 2^n\delta_n$. 

\paragraph*{Example problem} 
http://www.scribd.com/doc/49547850/Model-Building-in-Mathematical-Programming

\pagebreak
\section{How this could be solved}
We're trying to create an mixed integer linear programming model (MILP). In the original paper, they had one non-linear constraint:
\begin{align}
  (d_{max}-d_j)(1-x_{jk}) \geq D_k-d_j \;\;\;\forall j \in J, \forall k \in K
\end{align}
This can easily be turned into the simpler
\begin{align}
  D_k - (d_j - d_{max})x_{jk} \leq d_{max} \;\;\;\forall j \in J, \forall k \in K
\end{align}
\subsection{Possible improvements}
We cannot modify the solution process itself -- that's the whole point of the exercise, after all. Maybe we can preprocess to limit the domains of some decision variables. This is where ``preordering'' may come into play.
\paragraph{Limiting batch due date}
Since we start with as many batches as jobs, the solver is free to put every job into a batch of the same number. To make things more efficient, it can also put jobs into batches before, but it would never make sense to put a job into a batch with a higher number, so we have a limit that
\begin{align}
  x_{jk} = 0 \;\;\; \forall j,k: j > k
\end{align}


\end{document}

